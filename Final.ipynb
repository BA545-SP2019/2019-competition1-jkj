{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Competition #1 - Juliana Hoffmann, Kendall Stopa & Jonathan Uy**\n",
    "\n",
    "Folder with everything listed in this notebook is **Pipeline_1_(Z-Score)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics - Part 1\n",
    "\n",
    "Notebook: [Descriptive and Impute](./Pipeline_1_(Z-Score)/Descriptive_Impute.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration in Excel**\n",
    "\n",
    "To handle our data in Jupyter Lab, we first wanted to explore the data in Excel. \n",
    "Here we discovered many missing values scattered across the Excel that we would handle within the notebook through imputation. \n",
    "The missing values we did handle were the 8 missing values in the the I3 columns. The numbers listed here are SIC codes which we looked up through SICCODE.com to fill in the blanks and manually type them in the speadsheet. Next, in the I3 column, we discovered a handful of NAICS codes which we translated to the SIC code equivalent. Lastly, for the values in the I3 column that had multiple SIC codes per row, we used SIC Code to determine the primary SIC Code and imputated those manually in Excel. \n",
    "\n",
    "Later on through our competition notebooks, we discovered that there were -1 values that could not be ignored to calculate the ratio values later on in the notebook. In T5 (Row 7) and S1 (Row 134) we manually imputed those in the Excel using Jupyter to calculate the median of each column which wasused in place of the -1. \n",
    "\n",
    "Lastly, the names of the columns such as P(IPO) used parantheses in the title. We ran into issues creating new columns from these columns that containted parantheses in the title. To avoid such issues, we alter the names from P(IPO) to P_IPO, P(H) to P_H, P(L) to P_L and lastly P(1Day) to P_1DAY. \n",
    "\n",
    "These are the only changes that we manually made to the Excel file. These changes are reflected in the Excel: [Competition1_raw_data_fixed](./Pipeline_1_(Z-Score)/Competition1_raw_data_fixed.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration in Jupyter Lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we imported our CSV file as DataFrame, we began to explore the data within Jupyter Lab. Using .info() we found the the '-' used to identify a null value in the DataFrame was not being recognized as a null entry. To resolve this issue, we replaced the '-' with NaN. Once running the .info() again, the NaN entries were being identified as null values. \n",
    "We imputed the missing values using the Median except for using Mode for C2 as that was binary. \n",
    "We also imputed T2 with the median when there were \"0\" because to create the new ratio values, this column could not have 0 since it is the denominator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation of New Columns**\n",
    "\n",
    "After fully imputing missing values, zeros in T2 and -1 in T5 and S1, we were ready to create our new ratio values which included C3', C5', C6', T3', T4', T5', S1', S2' and S3'. Once created we dropped C3, C5, C6, T1-T5 and S1-S3. \n",
    "\n",
    "After including some visuals of the histrograms and box plots of the columns, we saved the DataFrame into the CSV titled [WorkingDataFrame](./Pipeline_1_(Z-Score)/WorkingDataFrame). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Skewness - Part 2 \n",
    "\n",
    "Notebook: [Skew1](./Pipeline_1_(Z-Score)/Skew1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before handling the skewness, we removed the 'target' variables from the DataFrame to make our Data Cleaning process easier. These are moved into its own DataFrame titled [Targets](./Pipeline_1_(Z-Score)/Targets). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the original skewness of the imputed data, we can determine which columns need to be alter and which can remain unchanged. We immediately know we can avoid changing C2 and C3' variables as they are binary. We also see that C4 and T4' are already properly skewed and will not require any changes. \n",
    "\n",
    "Otherwise, each columns' skewness was fixed on a column by column basis whether that required ths use of log, square root, cube root, exponents or a combination of a couple different methods chained together. We visualize the fixed shews through histograms of the altered data. \n",
    "\n",
    "Our goal is to get every skewness within a range of (-0.5, 0.5). For some cases, we were only able to get the the data within a range of (-1, 1) to have a moderate skew. \n",
    "\n",
    "The fixed skew data can be found in the CSV [SkewDF1](./Pipeline_1_(Z-Score)/SkewDF1). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying and Handling Outliers - Part 3\n",
    "\n",
    "Notebook: [STD_Dev_Outlier](./Pipeline_1_(Z-Score)/STD_Dev_Outlier.ipynb) \n",
    "\n",
    "Since we are not assuming normal distribution, we are handling outliers by identifying values that are 3 standard deviations below or 3 standard deviations above the mean. These outliers are then being replaced with the median of the column. \n",
    "We were able to accomplish this by creating a function then applying it to the columns that the function was applicable to. We skipped over C2 and C3' since those variables are binary. \n",
    "We visualized the fixed outliers through box plots of the altered data.\n",
    "\n",
    "The fixed outlier data can be found in the CSV [OutlierFix1](./Pipeline_1_(Z-Score)/OutlierFix1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization - Part 4\n",
    "\n",
    "Notebook: [Normalization](./Pipeline_1_(Z-Score)/Normalization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data was handled in this pipeline under the assumption that the data is not normally distrubuted, we are normalizing our data after handling skewness and outliers by using the 'Z-Score Method.'\n",
    "This entails using the .StandardScaler function from Sklearn.Preprocessing to remove the mean and scale to unit variance. \n",
    "\n",
    "The results of the normalization are visualized through histograms demonstrating the now normally distributed data. Again, this function was not applied to C2 and C3' as they are binary. \n",
    "\n",
    "The normalized data can be found in the CSV: [CompDF1](./Pipeline_1_(Z-Score)/CompDF1). \n",
    "\n",
    "Within this notebook, we concatted our normalized data with those target variables we temporarily removed way back in the Skew1 Notebook. \n",
    "Now, the data reessembled in the [ReadyDF1](./Pipeline_1_(Z-Score)/ReadyDF1) CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning Target Variables - Part 5\n",
    "\n",
    "Notebook: [Binning](./Pipeline_1_(Z-Score)/Binning.ipynb)\n",
    "\n",
    "Here we explored binning the I3 values. As mentioned at the top of this notebook, the I3 column contains the relevant SIC codes for the companies listed within this dataset. We cleaned these values within Excel and Python. \n",
    "\n",
    "While SIC Codes do have 10 designated divisions, we wanted to narrow those down to reflect 3 bins. Through counts of ranges we determined the most popular SIC Codes in the dataset. This included Manufacturing falling between 2000-3000 and Services being anything above 7000 in our data. We made these two separate bins. For our third bin we gathered all the other SIC codes from the remaining divisions to create an Other bin. \n",
    "Our Manufacturing Bin had 308 attributes, Services had 209 while Other had 165 attributes. \n",
    "\n",
    "Our binned Target Variable data can be found in the CSV: [Binning](./Pipeline_1_(Z-Score)/Binning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Correlation - Part 6 \n",
    "Notebook: [FilterCorrelation1](./Pipeline_1_(Z-Score)/FilterCorrelation1.ipynb)\n",
    "\n",
    "For our first attempt we performed a filtered feature selection using a Spearman correlation. We created a heatmap matrix of correlation score between the features and the label. We then checked if the features are highly correlated with are target variables Y1 and Y2 using the thresholds (>.5 or <-.5). The features are then selected on the basis of their scores. \n",
    "\n",
    "This method did not produce a feasible outcome for feature selection for this data. The only conclusive information we obtained from this method was that C6’ was too correlated with target variable Y1 because it essentially is derived from the same source as Y1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE for Non-Binned Predictors - Part 7 \n",
    "\n",
    "Notebook: [RFE1](./Pipeline_1_(Z-Score)/RFE1.ipynb)\n",
    "\n",
    "The next method for feature selection that we used is Recursive Feature Eliminate (RFE).  This method fits an entire model and then removes the weakest feature/features until the specified number of features is reached. The predictors are ranked by its importance to the model. \n",
    "For Y1, C6' was taken out of the predictor values because of its high correlation to Y1. \n",
    "When we ran the RFE, we recorded the rankings for all the variables and ran the evaluation code for each of the feature selection. \n",
    "\n",
    "**The Top 3 Features for Y1 are: C4, T4' and S2'**\n",
    "\n",
    "**The Top 3 Features for Y2 are: C4, C6' and S2'**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning Predictor Variables - Part 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook: [Binning_Pre](./Pipeline_1_(Z-Score)/Binning_Pre.ipynb)\n",
    "\n",
    "Through the usage of KBinsDiscretizer and OneHotEncoder, we binned C4, T4', C6' AND S2'. \n",
    "Note: for KBinsDiscretizer to run in Jupyter Lab, we had to download 'pip install --user -U scikit-learn' in the Terminal.\n",
    "\n",
    "The results can be found in the CSV: [BinDF1](./Pipeline_1_(Z-Score)/BinDF1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE for Binned Values - Part 9 \n",
    "\n",
    "Notebook: [RFE1-Binned](./Pipeline_1_(Z-Score)/RFE1-Binned.ipynb)\n",
    "\n",
    "Once we binned the top 3 predictor values for each Y1 and Y2, we ran those through the RFE to find which binned column stood out the most against the others so that we can run them through the Evaluation Code again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code and Conclusion\n",
    "\n",
    "Notebook: [Final Eval Code](./Pipeline_1_(Z-Score)/Evaluation-Code-FINAL.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Y1 we’ve come to the conclusion that the best 6 variables produce the best F1 and AUC score. For Y2 our best F1 and AUC score was produced by binning variables prior to running the RFE and using the best 5 variables in our evaluation code. \n",
    "We discovered that Y1 produced better results when the top features were NOT binned. Meanwhile Y2 produced the better results when the top feautures were binned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eval Best Run:**\n",
    "    \n",
    "    Y1 with C4, T4', S2', T5', S1' AND C2 (6 Variables)\n",
    "        F1 = 0.6055\n",
    "        AUC = 0.6041\n",
    "    \n",
    "    Y2 with C6'_1, C6'_2, S2'_1, C4_1, S2'_3 (5 Variables)\n",
    "        F1 = 0.6435\n",
    "        AUC = 0.6629\n",
    "    \n",
    "\n",
    "Our Final CSV with all the necessary values is [FINALDF](./Pipeline_1_(Z-Score)/FINALDF) within the Pipeline 1 Folder of our repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Pipelines and Feature Selection Methods\n",
    "\n",
    "Pipeline 2 - This pipeline assumed normal distribution. Therefore, we started with handling outliers with IQR, then normalizing data using MinMax then adjusting the Skew. While this did also produce strong results, we ultimately chose to go with Pipeline 1 since the numbers were a little better.\n",
    "\n",
    "Pipeline 3 - This pipeline is almost identical to pipeline 1 except that the values were imputed with the mean instead of the median. \n",
    "\n",
    "Pipeline 4 - This pipeline is almost identical to pipeline 2 except that the values were imputed with the mean instead of the median. \n",
    "\n",
    "Extra Trees Classifier - We also ran an extra trees classifier to report feature importances. The idea is whatever features passing the threshold would be selected and display the relative importance of each attribute. The features selected with the highest relative importance from this method gave us lower F1 and AUC, therefore we did not use this method in our final evaluation code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
